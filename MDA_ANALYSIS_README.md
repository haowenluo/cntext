# MD&A Text Analysis Pipeline

## Overview

This project provides a comprehensive end-to-end pipeline for analyzing Management Discussion & Analysis (MD&A) sections from SEC 10-K filings using the `cntext` library. It extracts 47+ financial text metrics across multiple dimensions.

## Features

### âœ… Complete Analysis Pipeline
- **Batch processing** with progress tracking
- **Error handling** and detailed logging
- **Sample and full dataset** support
- **Multiple output formats** (Parquet, CSV)

### ðŸ“Š 47+ Metrics Extracted

#### 1. Financial Sentiment (Loughran-McDonald Dictionary)
The gold standard for SEC filing analysis:
- Positive/Negative sentiment
- Uncertainty language
- Litigious tone
- Constraining language
- Modal strength (weak/moderate/strong)
- Net sentiment and polarity

#### 2. Emotion Analysis (NRC Dictionary)
- Anger, Fear, Joy, Sadness
- Anticipation, Disgust, Surprise, Trust

#### 3. Readability Indices (6 measures)
- Fog Index
- Flesch-Kincaid Grade Level
- SMOG Index
- Coleman-Liau Index
- Automated Readability Index (ARI)
- RIX Index

#### 4. Vocabulary Features
- Word count and unique words
- Type-Token Ratio (vocabulary diversity)
- Hapax legomena rate
- HHI concentration index
- Average word/sentence length

#### 5. Text Metadata
- Character counts
- Sentence counts
- Stopword counts

## Files

### Core Scripts

1. **`generate_fake_mda_data.py`**
   - Generates realistic synthetic MD&A data for testing
   - Creates 5,576 fake filings (2008-2025)
   - Outputs: `mda_metadata.csv` and `mda_full.parquet`

2. **`analyze_mda_comprehensive.py`**
   - Main analysis pipeline
   - Processes MD&A texts and extracts all metrics
   - Outputs: Results files + summary statistics + log

### Documentation

- `README_ANALYSIS_GUIDES.md` - Navigation and overview
- `MDA_QUICK_REFERENCE.md` - Code examples and quick start
- `MDA_FEATURE_MATRIX.md` - Complete metrics reference
- `CNTEXT_COMPLETE_ANALYSIS.md` - Full technical documentation
- `EXPLORATION_SUMMARY.md` - Executive overview

## Quick Start

### 1. Install Dependencies

```bash
pip install pandas pyarrow numpy tqdm nltk jieba scikit-learn scipy gensim
pip install PyMuPDF python-docx pyyaml chardet opencc-python-reimplemented
pip install ftfy contractions distinctiveness prettytable psutil
pip install matplotlib seaborn spacy nest_asyncio anthropic openai
pip install instructor aiolimiter
```

### 2. Download NLTK Data

```python
import nltk
nltk.download('punkt')
nltk.download('punkt_tab')
```

### 3. Prepare Your Data

Your data should be in Parquet format with columns:
- `cik`: Company Central Index Key
- `company`: Company name
- `filing_date`: Filing date (YYYY-MM-DD)
- `period_of_report`: Fiscal period end date
- `year`: Fiscal year
- `mda_text`: Full MD&A text content

### 4. Run Analysis

#### Option A: Use Fake Data for Testing

```bash
# Generate synthetic dataset
python generate_fake_mda_data.py

# Run analysis on sample (100 records)
python analyze_mda_comprehensive.py
```

#### Option B: Analyze Your Real Data

Modify `analyze_mda_comprehensive.py`:

```python
# Change line 395:
analyzer.load_data(sample_size=None)  # Process all records

# Or specify sample size:
analyzer.load_data(sample_size=1000)  # Process 1000 records
```

Then run:

```bash
python analyze_mda_comprehensive.py
```

### 5. View Results

Output files:
- **`mda_analysis_results.parquet`** - Main results (recommended)
- **`mda_analysis_results.csv`** - Human-readable format
- **`mda_analysis_summary_stats.csv`** - Statistical summary
- **`mda_analysis.log`** - Detailed processing log

## Usage Examples

### Example 1: Basic Analysis

```python
from analyze_mda_comprehensive import MDAAnalyzer

# Initialize
analyzer = MDAAnalyzer(data_path='mda_full.parquet')

# Load data
analyzer.load_data(sample_size=100)

# Run analysis
analyzer.analyze_batch(batch_size=50)

# Export results
analyzer.export_results(output_path='results.parquet', format='both')

# Generate summary
analyzer.generate_summary_statistics()
```

### Example 2: Load and Explore Results

```python
import pandas as pd

# Load results
df = pd.read_parquet('mda_analysis_results.parquet')

# View summary statistics
print(df.describe())

# Top companies by positive sentiment
top_positive = df.nlargest(10, 'lm_positive_pct')[['company', 'year', 'lm_positive_pct']]
print(top_positive)

# Filter by uncertainty
high_uncertainty = df[df['lm_uncertainty_pct'] > 2.0]
print(f"Filings with high uncertainty: {len(high_uncertainty)}")

# Year-over-year trends
yearly_sentiment = df.groupby('year')['lm_positive_pct'].mean()
print(yearly_sentiment)
```

### Example 3: Customized Analysis

```python
# Analyze specific companies
df_filtered = pd.read_parquet('mda_full.parquet')
df_apple = df_filtered[df_filtered['company'] == 'Apple Inc.']

# Save subset
df_apple.to_parquet('apple_mda.parquet')

# Run analysis on subset
analyzer = MDAAnalyzer(data_path='apple_mda.parquet')
analyzer.load_data()
analyzer.analyze_batch()
analyzer.export_results('apple_results.parquet')
```

## Performance

- **Speed**: ~2.4 filings/second (0.42 seconds per filing)
- **Sample (100 filings)**: ~42 seconds
- **Full dataset (5,576 filings)**: ~39 minutes estimated

## Output Schema

The results DataFrame contains 47 columns:

| Category | Columns | Example Metrics |
|----------|---------|-----------------|
| **Metadata** | 5 | cik, company, filing_date, year |
| **Text Stats** | 11 | char_count, word_num, sentence_num |
| **Readability** | 6 | fog_index, flesch_kincaid_grade, smog_index |
| **L&M Sentiment** | 17 | lm_positive_pct, lm_negative_pct, lm_uncertainty_pct |
| **NRC Emotions** | 8 | nrc_anger_pct, nrc_fear_pct, nrc_joy_pct |

## Key Metrics Interpretation

### Loughran-McDonald Sentiment
- **Positive**: 5-10% is typical
- **Negative**: 1-3% is typical
- **Uncertainty**: 1-2% is typical
- **Net Sentiment**: Positive - Negative (higher is more positive)

### Readability
- **Fog Index**: 12 = high school, 16 = college
- **Flesch-Kincaid**: Grade level required to understand
- **Higher values** = More complex text

### Vocabulary
- **Type-Token Ratio**: 0.4-0.6 is typical (higher = more diverse)
- **Hapax Rate**: Percentage of words appearing once

## Troubleshooting

### Common Issues

1. **Memory Error**
   - Solution: Process in smaller batches or use `sample_size` parameter

2. **Missing Dependencies**
   - Solution: See installation section and install all required packages

3. **Slow Processing**
   - Solution: Reduce batch size or use multiprocessing (future enhancement)

## Advanced Features

### Modify Batch Size

```python
# Larger batches = less frequent progress updates
analyzer.analyze_batch(batch_size=200)
```

### Custom Dictionaries

```python
# Load custom sentiment dictionary
from cntext.io.dict import read_yaml_dict
custom_dict = read_yaml_dict('my_custom_dict.yaml')

# Modify the analyzer to use custom dictionary
analyzer.lm_dict = custom_dict
```

### Export Format Options

```python
# Parquet only (smaller, faster)
analyzer.export_results(format='parquet')

# CSV only (human-readable)
analyzer.export_results(format='csv')

# Both formats
analyzer.export_results(format='both')
```

## Research Applications

This pipeline is ideal for:

1. **Financial Research**
   - Sentiment trends over time
   - Crisis period analysis (2008, 2020)
   - Industry comparisons

2. **Regulatory Studies**
   - Disclosure quality assessment
   - Readability compliance

3. **Machine Learning**
   - Feature extraction for predictive models
   - Training data preparation

4. **Corporate Communication**
   - Boilerplate detection
   - Information content analysis

## Citation

If you use this pipeline in your research, please cite:

```bibtex
@software{mda_analysis_pipeline,
  title = {MD&A Text Analysis Pipeline using cntext},
  author = {Your Name},
  year = {2025},
  url = {https://github.com/haowenluo/cntext}
}
```

## References

- **Loughran & McDonald Financial Dictionary**: [Paper](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=1331573)
- **cntext Library**: [GitHub](https://github.com/yourusername/cntext)
- **SEC EDGAR**: [Website](https://www.sec.gov/edgar)

## License

This project uses the cntext library which has its own license. Please review the license before commercial use.

## Support

For issues and questions:
- Check the documentation files in this repository
- Review the analysis log file for error details
- Open an issue on GitHub

## Future Enhancements

- [ ] Multiprocessing support for faster processing
- [ ] Semantic similarity analysis (year-over-year changes)
- [ ] Topic modeling integration
- [ ] Forward-looking statement detection
- [ ] Visualization dashboard
- [ ] Real-time processing pipeline

## Acknowledgments

- **cntext library** for providing comprehensive text analysis tools
- **Loughran & McDonald** for the financial sentiment dictionary
- **SEC** for making 10-K filings publicly available

---

**Last Updated**: November 19, 2025
**Version**: 1.0.0
**Status**: Production Ready âœ…
